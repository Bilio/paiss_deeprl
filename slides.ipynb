{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"paiss.jpg\" heigth=45>\n",
    "\n",
    "# Practical Session : Deep RL\n",
    "\n",
    "## Agenda\n",
    "\n",
    "  1. Deep-RL Theory Reminder (15')\n",
    "  1. DQN (30')\n",
    "  1. $\\epsilon$-DQN (20')\n",
    "  1. DQN with Replay (20')\n",
    "  1. Potential problems & solutions (10')\n",
    "  1. Wrap-up (5')\n",
    "  \n",
    "(!) Regular polls/quiz\n",
    "  \n",
    "  <img src=\"logo.png\" width=\"250px\" align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep-RL Theory\n",
    "   \n",
    "<img src=\"rl.png\" width=\"480\"/>   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Goal**: Maximise w.r.t $\\pi$ to collect more cumulated discounted reward \n",
    "  - $R = \\sum \\limits_{t=0}^{\\infty} \\gamma^t r_t$\n",
    "  - $0<\\gamma<1$\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning for a Markov Decision Process \n",
    "\n",
    "Under a fixed policy, $V$ is the value of the state:\n",
    "\n",
    "<img src=\"vpi.png\" width=\"480\"/>\n",
    "\n",
    "Under a fixed policy, $Q$ is the value of the tuple (state, action):\n",
    "\n",
    "<img src=\"qpi.png\" width=\"640\"/>\n",
    "\n",
    "Remark: after time $t$, actions are chosen by $\\pi$ so in expectation we could replace $Q^\\pi(s_{t+1}, a_{t+1})$ by $V^\\pi(s_{t+1})$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bellmanâ€˜s optimality principle\n",
    "\n",
    "The optimal policy $\\pi^*$ must respect \n",
    "\n",
    "<img src=\"bellman.png\" width=480>\n",
    "\n",
    "For a fixed policy $\\pi$ if the quantity are not equals :\n",
    "  - either $Q$ is not correctly estimated (policy evaluation issue) \n",
    "  - or $\\pi$ is not selecting optimal actions, this can be fixed being more greedy wrt $Q$\n",
    "\n",
    "<img src=\"evalimprov.png\" width=480>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Q-Learning (DQN)\n",
    "\n",
    "<img src=\"dqn-th.png\" width=800>\n",
    "\n",
    "<img src=\"q-learning.png\" width=480>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep-RL Benchmarks\n",
    "\n",
    "## OpenAI \n",
    "\n",
    "  - [Gym](https://gym.openai.com/envs/CartPole-v0/) : set of standard problems / environments\n",
    "\n",
    "### Simple control problems\n",
    "\n",
    "<img src=\"control.png\" width=480>\n",
    "\n",
    "### Famous learn to play Atari\n",
    "\n",
    "<img src=\"atari.png\" width=480>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class RLEnvironment:\n",
    "\n",
    "    def run(self, agent, episodes=100, ...):\n",
    "        \"\"\"\n",
    "        Run the agent.\n",
    "\n",
    "        Pseudo-code:\n",
    "        ```\n",
    "            for i in 1..episodes {\n",
    "\n",
    "                start new episode\n",
    "\n",
    "                while episode not finished {\n",
    "\n",
    "                    ask agent to take action based on current state\n",
    "                    action is resolved by environment, returning reward and new state\n",
    "\n",
    "                    <opportunity to feedback agent with (state, reward, new state)>\n",
    "                    <opportunity to update agent parameters>\n",
    "\n",
    "                    if new state means agent failed {\n",
    "                        terminate episode\n",
    "                    }\n",
    "\n",
    "                }\n",
    "\n",
    "                <opportunity to update agent parameters again>\n",
    "\n",
    "                if last episodes show enough reward {\n",
    "                    declare task solved\n",
    "                }\n",
    "\n",
    "            }\n",
    "\n",
    "        ```\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "env.run(RandomAgent(env.action_space), episodes=20, display_policy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### CartPole\n",
    "  - $x$: position of cart\n",
    "  - $\\theta$: angle of the pole\n",
    "  - actions: move left/right\n",
    "  - environment simulates acceleration (cart/pole mass)\n",
    "<img src=\"cartpole.gif\" width=640>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. DQN in practice\n",
    "  - [Keras doc](https://keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent(RLDebugger):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.learning_rate = ???\n",
    "        \n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # try different nb neurons    \n",
    "        # try different layers\n",
    "        model.add(Dense(10, input_dim=self.state_size, activation='linear'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        # usual activations: 'linear', 'relu', 'tanh', 'sigmoid'\n",
    "        model.compile(loss=???, optimizer=???(lr=self.learning_rate))\n",
    "        # usual losses: 'mse', 'logcosh', 'mean_absolute_error'\n",
    "        model.summary()\n",
    "        # usual optimizers: Adam, RMSprop, SGD\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    # get action from model using greedy policy. \n",
    "    def get_action(self, state):\n",
    "        q_value = self.model.predict(state)\n",
    "        best_action = np.argmax(q_value[0]) #The [0] is because keras outputs a set of predictions of size 1\n",
    "        return best_action\n",
    "\n",
    "    # train the target network on the selected action and transition\n",
    "    def train_model(self, action, state, next_state, reward, done):\n",
    "        target = self.model.predict(state)\n",
    "        # We use our internal model in order to estimate the V value of the next state \n",
    "        target_val = self.target_model.predict(next_state)\n",
    "        # Q Learning: target values should respect the Bellman's optimality principle\n",
    "        if done: #We are on a terminal state\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            target[0][action] = reward + self.gamma * (np.amax(target_val))\n",
    "\n",
    "        # and do the model fit!\n",
    "        loss = self.model.fit(state, target, verbose=0).history['loss'][0]\n",
    "        self.record(action, state, target, target_val, loss)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.observation_space, env.action_space)\n",
    "env.run(agent, episodes=100, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_1 (Dense)              (None, 30)                150       \n",
    "=================================================================\n",
    "Total params: 150\n",
    "Trainable params: 150\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Episode 10, Total reward 9.0\n",
    "Episode 20, Total reward 9.0\n",
    "Episode 30, Total reward 10.0\n",
    "Episode 40, Total reward 10.0\n",
    "Episode 50, Total reward 10.0\n",
    "Episode 60, Total reward 10.0\n",
    "Episode 70, Total reward 10.0\n",
    "Episode 80, Total reward 10.0\n",
    "Episode 90, Total reward 10.0\n",
    "Episode 100, Total reward 10.0\n",
    "Average Total Reward of last 100 episodes: 9.94\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Evaluation\n",
    "- **Objective:** average reward of 200 over 100 episodes\n",
    "- *First Target:* reach a reward > 50 (at least once)\n",
    "- We only focus on tuning model parameters for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "___\n",
    "\n",
    "Your turn now !\n",
    "\n",
    "```\n",
    "$ jupyter notebook exercises.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DQN First Results\n",
    "\n",
    "### POLL: who reached a reward > 50 ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Symptom\n",
    "\n",
    "![](loss_dqn_1.png)\n",
    "\n",
    "### POLL: Is it a problem of ...\n",
    "  - Model capacity (#neurons) ?\n",
    "  - Loss function ?\n",
    "  - Activation ?\n",
    "  - Exploration ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](action_dqn_1.png)\n",
    "\n",
    "*Take away:* Q-learning will converge only if it explores enough actions (hence states)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DQN with Exploration\n",
    "\n",
    "Exploration of the states is crucial for performance\n",
    "\n",
    "  - add an uniform exploration mechanism\n",
    "  - decrease exploration over time\n",
    "  \n",
    "This is our first agent which is going to solve the task. It will typically require to run a few hundred of episodes to collect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgentWithExploration(DQNAgent):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(DQNAgentWithExploration, self).__init__(observation_space, action_space)\n",
    "        # exploration schedule parameters \n",
    "        self.t = 0\n",
    "        self.epsilon = ???\n",
    "        # TODO store your additional parameters here \n",
    "\n",
    "    # decay epsilon\n",
    "    def update_epsilon(self):\n",
    "        self.t += 1\n",
    "        # TODO write the code for your decay  \n",
    "        self.epsilon = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "agent = DQNAgentWithExploration(env.observation_space, env.action_space)\n",
    "env.run(agent, episodes=500, print_delay=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_7 (Dense)              (None, 30)                150       \n",
    "_________________________________________________________________\n",
    "dense_8 (Dense)              (None, 2)                 62        \n",
    "=================================================================\n",
    "Total params: 212\n",
    "Trainable params: 212\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Episode 50, Total reward 10.0\n",
    "Episode 100, Total reward 37.0\n",
    "Episode 150, Total reward 139.0\n",
    "Episode 200, Total reward 200.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___\n",
    "\n",
    "Your turn now !\n",
    "\n",
    "PS: if your current DQN is really poor (not reaching reward > 10) you can cheat by using:\n",
    "```\n",
    "from .spoiler import DQNAgent\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DQN/Explore Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### POLL: who reached a reward > 50 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### POLL: who reached a reward > 200 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### POLL: which one is DQN ? DQN/Ex ?\n",
    "| | |\n",
    "|---|--- |\n",
    "| ![](state_dqn_2.png) | ![](state_dqne_1.png) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### POLL: who got the message \"task solved\" ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### POLL: who solved in < 200 episodes ?\n",
    "\n",
    "*hint*: can we optimize gains from past experience ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 7. DQN/Ex with Replay\n",
    "\n",
    "  - So we collect data according to games recorded using a policy which contains an exploration mechanism.\n",
    "\n",
    "  - The collected data are going to be highly correlated\n",
    "  \n",
    "  - This is a good idea to fight it by keeping a common memory along several episodes and to subsample the training batches in this memory.  \n",
    " \n",
    "  - Prioritized Replay - https://arxiv.org/abs/1511.05952 - goes one step further by weighting the sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgentWithExplorationAndReplay(DQNAgentWithExploration):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        ...\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=???)\n",
    "        self.batch_size = ???\n",
    "\n",
    "    def train_model(self, action, state, next_state, reward, done):\n",
    "        \n",
    "        # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if len(self.memory) >= self.train_start:\n",
    "            ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "___\n",
    "\n",
    "Your turn now !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DQN/Ex/Replay Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### POLL: who got the message \"task solved\" ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### POLL: who solved in < 200 episodes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### POLL: Have we converged ?\n",
    "\n",
    "| | |\n",
    "|--|--|\n",
    "| ![](dqnee_state_1.png) | ![](dqnee_bellman_1.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### POLL: Have we converged ?\n",
    "\n",
    "![](dqnee_all_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "agent.epsilon = 0\n",
    "agent.memory = deque(maxlen=1)\n",
    "agent.batch_size = 1\n",
    "agent.train_start = 1\n",
    "env.run(agent, episodes=200, print_delay=33)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Potential Problems/Solution (advanced)\n",
    "\n",
    "## Double DQN\n",
    "  - *assumption*: our Q estimates are too optimistic\n",
    "  - *solution*: defer model update to avoid big jumps in target\n",
    "  \n",
    "  <img src=\"ddqn-th.png\" width=480>\n",
    "\n",
    "  - practically: \"freeze\" $Q_2$ for several time steps / episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dueling DQN\n",
    "  - *assumption*: action is not so important for many states\n",
    "  - *solution*: separate $Q$ into state and advantage functions $Q(s,a) = V(s) + A(s,a)$\n",
    "  \n",
    "  <img src=\"duel.png\" width=140>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "___\n",
    "\n",
    "You can try to implement one of these as an exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Takeaways\n",
    "\n",
    "### Basics ideas\n",
    "  - tune the DL model parameters\n",
    "  - exploration is necessary for Q-learning\n",
    "  - debug not only with the model loss\n",
    "  \n",
    "### Advanced ideas\n",
    "  - make exploration more efficient (replay)\n",
    "  - adapt to task specifics (DDQN, Dueling DQN)\n",
    "  \n",
    "### Next steps\n",
    "  - [John Schulman: nuts and bolts of RL research](http://rll.berkeley.edu/deeprlcourse/docs/nuts-and-bolts.pdf)\n",
    "  - try the Atari gyms... with a GPU :)\n",
    "  - try Policy Gradient methods"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
