{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PAISS Practical Deep-RL by Criteo Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "from utils import RLEnvironment, RLDebugger\n",
    "\n",
    "import random\n",
    "\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input, Reshape, Lambda, Add, RepeatVector\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RLEnvironment()\n",
    "print(env.observation_space, env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play loop\n",
    "Note that this Gym environment is considered as solved as soon as you find a policy which scores 200 on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.run(RandomAgent(env.action_space), episodes=20, display_policy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## DQN Agent - Online\n",
    "Here is a keras code for training a simple DQN. \n",
    "\n",
    "It is presented first for the sake of clarity. Nevertheless, the trained network is immediatly used to collect the new training data, unless you are lucky you won't be able to find a way to solve the task. \n",
    "\n",
    "Just replace the `???` by some parameters which seems reasonnable to you ($\\gamma>1$ is not reasonnable and big steps are prone to numerical instability) and watch the failure of the policy training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RLDebugger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ca5e14b62092>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRLDebugger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mRLDebugger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# get size of state and action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RLDebugger' is not defined"
     ]
    }
   ],
   "source": [
    "class DQNAgent(RLDebugger):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        RLDebugger.__init__(self)\n",
    "        # get size of state and action\n",
    "        self.state_size = observation_space.shape[0]\n",
    "        self.action_size = action_space.n\n",
    "        # hyper parameters \n",
    "        self.learning_rate = ??? # recommended value range: [1e-3, 1e-1]\n",
    "        self.model = self.build_model()  \n",
    "        self.target_model = self.model\n",
    "        \n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # try adding neurons. Recommended value range [10, 256]  \n",
    "        # try adding layers. Recommended value range [1, 4]\n",
    "        model.add(Dense(units=10, input_dim=self.state_size, activation='linear'))\n",
    "        model.add(Dense(units=self.action_size, activation='linear'))\n",
    "        # usual activations: 'linear', 'relu', 'tanh', 'sigmoid'\n",
    "        model.compile(loss=???, optimizer=Adam(lr=self.learning_rate))\n",
    "        # usual losses: 'mse', 'logcosh', 'mean_absolute_error'\n",
    "        # usual optimizers: Adam, RMSprop, SGD\n",
    "        model.summary() # Display summary of the network. \n",
    "                        # Check that your network contains a \"reasonable\" number of parameters (a few hundrers)\n",
    "        return model\n",
    "\n",
    "    # get action from model using greedy policy. \n",
    "    def get_action(self, state):\n",
    "        q_value = self.model.predict(state)\n",
    "        best_action = np.argmax(q_value[0]) #The [0] is because keras outputs a set of predictions of size 1\n",
    "        return best_action\n",
    "\n",
    "    # train the target network on the selected action and transition\n",
    "    def train_model(self, action, state, next_state, reward, done):\n",
    "        target = self.model.predict(state)\n",
    "        # We use our internal model in order to estimate the V value of the next state \n",
    "        target_val = self.target_model.predict(next_state)\n",
    "        # Q Learning: target values should respect the Bellman's optimality principle\n",
    "        if done: #We are on a terminal state\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            target[0][action] = reward + self.gamma * (np.amax(target_val))\n",
    "\n",
    "        # and do the model fit!\n",
    "        loss = self.model.fit(state, target, verbose=0).history['loss'][0]\n",
    "        self.record(action, state, target, target_val, loss, reward)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.observation_space, env.action_space)\n",
    "env.run(agent, episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let-s try with a fixed initial position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.observation_space, env.action_space, seed=0)\n",
    "env.run(agent, episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent with Exploration\n",
    "This is our first agent which is going to solve the task. It will typically require to run a few hundred of episodes to collect the data. \n",
    "\n",
    "The difference with the previous agent is that you are going to add an exploration mechanism in order to take care of the data collection for the training. We advise to use an $\\varepsilon_n$-greedy, meaning that the value of $\\varepsilon$ is going to decay over time. Several kind of decays can be found in the litterature, a simple one is to use a mutiplicative update of $\\varepsilon$ by a constant smaller than 1 as long as $\\varepsilon$ is smaller than a small minimal rate (typically in the range 1%-5%).\n",
    "\n",
    "You need to:\n",
    "* Code your exploration (area are tagged in the code by some TODOs).\n",
    "* Tune the hyperparameters (including the ones from the previous section) in order to solve the task. This may be not so easy and will likely require more than 500 episodes and a final small value of epsilon. Next sessions will be about techniques to increase sample efficiency (i.e require less episodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgentWithExploration(DQNAgent):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(DQNAgentWithExploration, self).__init__(observation_space, action_space)\n",
    "        # exploration schedule parameters \n",
    "        self.t = 0\n",
    "        self.epsilon = ???\n",
    "\n",
    "    # decay epsilon\n",
    "    def update_epsilon(self):\n",
    "        # TODO write the code for your decay  \n",
    "        self.t += 1\n",
    "        self.epsilon = ???\n",
    "\n",
    "    # get action from model using greedy policy\n",
    "    def get_action(self, state):\n",
    "        # exploration \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_value = self.model.predict(state)\n",
    "        return np.argmax(q_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgentWithExploration(env.observation_space, env.action_space)\n",
    "env.run(agent, episodes=500, print_delay=50, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent with Exploration and Experience Replay\n",
    "\n",
    "We are now going to save some samples in a limited memory in order to build minibatches during the training. The exploration policy remains the same than in the previous section.  Storage is already coded you just need to modify the tagged section which is about building the mini-batch sent to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class DQNAgentWithExplorationAndReplay(DQNAgentWithExploration):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(DQNAgentWithExplorationAndReplay, self).__init__(observation_space, action_space)\n",
    "        self.batch_size = ???\n",
    "        self.train_start = ???\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=???)\n",
    "\n",
    "    def create_minibatch(self):\n",
    "        # pick samples randomly from replay memory (using batch_size)\n",
    "        \n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = np.array([_[0][0] for _ in samples])\n",
    "        actions = np.array([_[1] for _ in samples])\n",
    "        rewards = np.array([_[2] for _ in samples])\n",
    "        next_states = np.array([_[3][0] for _ in samples])\n",
    "        dones = np.array([_[4] for _ in samples])\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "        \n",
    "    def train_model(self, action, state, next_state, reward, done):\n",
    "        \n",
    "        # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if len(self.memory) >= self.train_start:\n",
    "            states, actions, rewards, next_states, dones = self.create_minibatch()\n",
    "            \n",
    "            targets = self.model.predict(states)\n",
    "            target_values = self.target_model.predict(next_states)\n",
    "\n",
    "            for i in range(self.batch_size):\n",
    "                # Approx Q Learning\n",
    "                if dones[i]:\n",
    "                    targets[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    targets[i][actions[i]] = rewards[i] + self.gamma * (np.amax(target_values[i]))\n",
    "           \n",
    "            # and do the model fit!\n",
    "            loss = self.model.fit(states, targets, verbose=0).history['loss'][0]\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                self.record(actions[i], states[i], targets[i], target_values[i], loss, rewards[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgentWithExplorationAndReplay(env.observation_space, env.action_space)\n",
    "env.run(agent, episodes=300, print_delay=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_bellman_residual()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN Agent with Exploration and Experience Replay\n",
    "\n",
    "Now we want to have two identical networks and keep frozen for some timesteps the one which is in charge of the evaluation (*i.e* which is used to compute the targets).\n",
    "Note that you can find some variants where the target network is updated at each timestep but with a small fraction of the difference with the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgentWithExplorationAndReplay(DQNAgentWithExplorationAndReplay):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(DoubleDQNAgentWithExplorationAndReplay, self).__init__(observation_space, action_space)\n",
    "        # TODO: initialize a second model\n",
    "        self.target_model = ???\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from the model used for action selection to the model used for computing targets\n",
    "        self.target_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DoubleDQNAgentWithExplorationAndReplay(env.observation_space, env.action_space)\n",
    "env.run(agent, episodes=200, print_delay=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_diagnostics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe actual performance of the policy we should set $\\varepsilon=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.epsilon = 0\n",
    "agent.memory = deque(maxlen=1)\n",
    "agent.batch_size = 1\n",
    "agent.train_start = 1\n",
    "env.run(agent, episodes=200, print_delay=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_diagnostics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Duelling DQN \n",
    "\n",
    "If time allows, adapt the description from http://torch.ch/blog/2016/04/30/dueling_dqn.html to our setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDuelingDQNAgentWithExplorationAndReplay(DoubleDQNAgentWithExplorationAndReplay):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "         super(DoubleDuelingDQNAgentWithExplorationAndReplay, self).__init__(observation_space, action_space)\n",
    "            \n",
    "    def build_model(self):\n",
    "        value_input = Input(shape=(self.state_size,))\n",
    "        \n",
    "        # Value stream\n",
    "        value_stream_hidden = Dense(???, input_dim=self.state_size, activation=???)(value_input)\n",
    "        value_stream_activation = Dense(1, activation=???)(value_stream_hidden)\n",
    "        repeat_value_stream = RepeatVector(self.action_size)(value_stream_activation)\n",
    "        value_stream = Flatten()(repeat_value_stream)\n",
    "        \n",
    "        # Advantage stream\n",
    "        advantage_stream_hidden = Dense(???, input_dim=self.state_size, activation=???)(value_input)\n",
    "        advantage_stream_activation = Dense(self.action_size, activation=???)(advantage_stream_hidden)\n",
    "        advantage_stream = Lambda(lambda layer: layer - K.mean(layer))(advantage_stream_activation)\n",
    "        \n",
    "        # Merge both streams\n",
    "        q_values = Add()([value_stream, advantage_stream])\n",
    "        \n",
    "        model = Model(inputs=[value_input], outputs=q_values)\n",
    "        \n",
    "        model.compile(loss=???, optimizer=???(lr=self.learning_rate))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DoubleDuelingDQNAgentWithExplorationAndReplay(env.observation_space, env.action_space)\n",
    "env.run(agent, episodes=300, print_delay=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_diagnostics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More ideas\n",
    "* Use an other environment providing pictures, replace the network by a pre-trained ResNet-50 and learn a policy to play your favorite Atari game (can easily take days of computation if not done asynchroneously on several V100 as described by https://github.com/dgriff777/rl_a3c_pytorch),\n",
    "* Visualize important areas of the pictures for taking the decision using saliency maps computed by guided backpropagation as shown by http://arxiv.org/abs/1412.6806.\n",
    "* Go to policy gradient methods "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
