{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAISS Practical Deep-RL by Criteo Research (Pytorch version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "\n",
    "from utils import RLEnvironment, RLDebugger\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = RLEnvironment()\n",
    "print(env.observation_space, env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play loop\n",
    "Note that this Gym environment is considered as solved as soon as you find a policy which scores 200 on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.run(RandomAgent(env.action_space), episodes=20, display_policy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## DQN Agent - Online\n",
    "Here is a keras code for training a simple DQN. \n",
    "\n",
    "It is presented first for the sake of clarity. Nevertheless, the trained network is immediatly used to collect the new training data, unless you are lucky you won't be able to find a way to solve the task. \n",
    "\n",
    "Just replace the `???` by some parameters which seems reasonnable to you ($\\gamma>1$ is not reasonnable and big steps are prone to numerical instability) and watch the failure of the policy training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, ???)\n",
    "        self.fc2 = nn.Linear(???, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.???(self.fc1(x)) # non-linear activation\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent(RLDebugger):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        RLDebugger.__init__(self)\n",
    "        # get size of state and action\n",
    "        self.state_size = observation_space.shape[0]\n",
    "        self.action_size = action_space.n\n",
    "        # hyper parameters \n",
    "        self.gamma = ???\n",
    "        self.learning_rate = ???\n",
    "        self.build_model()  \n",
    "        self.target_model = self.model\n",
    "        \n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        self.model = Model(input_dim=self.state_size, output_dim=self.action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.loss = nn.???Loss()\n",
    "        # 1/ You can try different losses. As an logcosh loss is a twice differenciable approximation of Huber loss\n",
    "        # 2/ From a theoretical perspective Learning rate should decay with time to guarantee convergence \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        q_value = self.model(state).detach().numpy()\n",
    "        best_action = np.argmax(q_value[0]) #The [0] is because keras outputs a set of predictions of size 1\n",
    "        return int(best_action)\n",
    "    \n",
    "    # train the target network on the selected action and transition\n",
    "    def train_model(self, action, state, next_state, reward, done):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        next_state = torch.from_numpy(next_state).float()\n",
    "        \n",
    "        val = self.model(state)[0][action]\n",
    "        target = self.model(state)\n",
    "        target_val = self.target_model(next_state)\n",
    "        \n",
    "        if done: #We are on a terminal state\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            target[0][action] = reward + self.gamma * (torch.max(target_val))\n",
    "        \n",
    "        # and do the model fit!\n",
    "        self.model.zero_grad()\n",
    "        loss = self.loss(val, target.detach()[0][action])\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.record(action, state, target, target_val, loss, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.observation_space, env.action_space)\n",
    "env.run(agent, episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try with a fixed initial position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.observation_space, env.action_space)\n",
    "env.run(agent, episodes=300, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent with Exploration\n",
    "This is our first agent which is going to solve the task. It will typically require to run a few hundred of episodes to collect the data. \n",
    "\n",
    "The difference with the previous agent is that you are going to add an exploration mechanism in order to take care of the data collection for the training. We advise to use an $\\varepsilon_n$-greedy, meaning that the value of $\\varepsilon$ is going to decay over time. Several kind of decays can be found in the litterature, a simple one is to use a mutiplicative update of $\\varepsilon$ by a constant smaller than 1 as long as $\\varepsilon$ is smaller than a small minimal rate (typically in the range 1%-5%).\n",
    "\n",
    "You need to:\n",
    "* Code your exploration (area are tagged in the code by some TODOs).\n",
    "* Tune the hyperparameters (including the ones from the previous section) in order to solve the task. This may be not so easy and will likely require more than 500 episodes and a final small value of epsilon. Next sessions will be about techniques to increase sample efficiency (i.e require less episodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgentWithExploration(DQNAgent):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(DQNAgentWithExploration, self).__init__(observation_space, action_space)\n",
    "        # exploration schedule parameters \n",
    "        self.t = 0\n",
    "        self.epsilon = ??? # Designs the probability of taking a random action. \n",
    "                           # Should be in range [0,1]. The closer to 0 the greedier. \n",
    "                           # Hint: start close to 1 (exploration) and end close to zero (exploitation).\n",
    "\n",
    "    # decay epsilon\n",
    "    def update_epsilon(self):\n",
    "        # TODO write the code for your decay\n",
    "        self.t += 1\n",
    "        self.epsilon = ???\n",
    "\n",
    "    # get action from model using greedy policy\n",
    "    def get_action(self, state):\n",
    "        # exploration \n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        state = torch.from_numpy(state).float()\n",
    "        q_value = self.model(state).detach().numpy()\n",
    "        best_action = np.argmax(q_value[0])\n",
    "        return int(best_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = DQNAgentWithExploration(env.observation_space, env.action_space)\n",
    "env.run(agent, episodes=500, print_delay=50, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.plot_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent with Exploration and Experience Replay\n",
    "\n",
    "We are now going to save some samples in a limited memory in order to build minibatches during the training. The exploration policy remains the same than in the previous section.  Storage is already coded you just need to modify the tagged section which is about building the mini-batch sent to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class DQNAgentWithExplorationAndReplay(DQNAgentWithExploration):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(DQNAgentWithExplorationAndReplay, self).__init__(observation_space, action_space)\n",
    "        self.batch_size = ??? # Recommended value range [10, 1000]\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=10000) # Recommended value range [10, 20000]\n",
    "\n",
    "    def create_minibatch(self):\n",
    "        # pick samples randomly from replay memory (using batch_size)\n",
    "        \n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        samples = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        states = np.array([_[0][0] for _ in samples])\n",
    "        actions = np.array([_[1] for _ in samples])\n",
    "        rewards = np.array([_[2] for _ in samples])\n",
    "        next_states = np.array([_[3][0] for _ in samples])\n",
    "        dones = np.array([_[4] for _ in samples])\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "        \n",
    "    def train_model(self, action, state, next_state, reward, done):\n",
    "        \n",
    "        # save sample <s,a,r,s'> to the replay memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        if len(self.memory) >= self.batch_size:\n",
    "            states, actions, rewards, next_states, dones = self.create_minibatch()\n",
    "            \n",
    "            states = torch.from_numpy(states).float()\n",
    "            next_states = torch.from_numpy(next_states).float()\n",
    "\n",
    "            vals = self.model(states).gather(1,torch.from_numpy(actions).view(-1,1))\n",
    "            targets = self.model(states)\n",
    "            targets_val = self.target_model(next_states).detach()\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                # Approx Q Learning\n",
    "                if dones[i]:\n",
    "                    targets[i][actions[i]] = rewards[i]\n",
    "                else:\n",
    "                    targets[i][actions[i]] = rewards[i] + self.gamma * (torch.max(targets_val[i]))\n",
    "           \n",
    "            # and do the model fit!\n",
    "            self.model.zero_grad()\n",
    "            loss = self.loss(vals, targets.detach().gather(1,torch.from_numpy(actions).view(-1,1)))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            for i in range(self.batch_size):\n",
    "                self.record(actions[i], states[i], targets[i], targets_val[i], loss / self.batch_size, rewards[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = DQNAgentWithExplorationAndReplay(env.observation_space, env.action_space)\n",
    "env.run(agent, episodes=300, print_delay=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.plot_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.plot_bellman_residual()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN Agent with Exploration and Experience Replay\n",
    "\n",
    "Now we want to have two identical networks and keep frozen for some timesteps the one which is in charge of the evaluation (*i.e* which is used to compute the targets).\n",
    "Note that you can find some variants where the target network is updated at each timestep but with a small fraction of the difference with the policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DoubleDQNAgentWithExplorationAndReplay(DQNAgentWithExplorationAndReplay):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(DoubleDQNAgentWithExplorationAndReplay, self).__init__(observation_space, action_space)\n",
    "        # TODO: initialize a second model\n",
    "        self.target_model = Model(input_dim=self.state_size, output_dim=self.action_size)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # copy weights from the model used for action selection to the model used for computing targets\n",
    "        self.target_model.load_state_dict(self.model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = DoubleDQNAgentWithExplorationAndReplay(env.observation_space, env.action_space)\n",
    "env.run(agent, episodes=300, print_delay=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.plot_diagnostics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe actual performance of the policy we should set $\\varepsilon=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.epsilon = 0\n",
    "agent.memory = deque(maxlen=1)\n",
    "agent.batch_size = 1\n",
    "env.run(agent, episodes=300, print_delay=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.plot_diagnostics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duelling DQN \n",
    "\n",
    "If time allows, adapt the description from http://torch.ch/blog/2016/04/30/dueling_dqn.html to our setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DuelingModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DuelingModel, self).__init__()\n",
    "        self.action_dim = output_dim\n",
    "        self.value_fc1 = nn.Linear(input_dim, ???)\n",
    "        self.value_fc2 = nn.Linear(???, 1)\n",
    "        \n",
    "        self.advantage_fc1 = nn.Linear(input_dim, ???)\n",
    "        self.advantage_fc2 = nn.Linear(???, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent_values = F.tanh(self.value_fc1(x))\n",
    "        value = self.value_fc2(latent_values)\n",
    "        value_repeat = torch.cat([value]*self.action_dim, 1)\n",
    "        \n",
    "        latent_advantages = F.tanh(self.advantage_fc1(x))\n",
    "        advantage = self.advantage_fc2(latent_advantages)\n",
    "        \n",
    "        q_values = advantage + value_repeat\n",
    "        return q_values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DoubleDuelingDQNAgentWithExplorationAndReplay(DoubleDQNAgentWithExplorationAndReplay):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(DoubleDuelingDQNAgentWithExplorationAndReplay, self).__init__(observation_space, action_space)\n",
    "        self.target_model = DuelingModel(input_dim=self.state_size, output_dim=self.action_size)\n",
    "            \n",
    "    def build_model(self):\n",
    "        self.model = DuelingModel(input_dim=self.state_size, output_dim=self.action_size)\n",
    "        self.optimizer = optim.???(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.loss = nn.???Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent = DoubleDuelingDQNAgentWithExplorationAndReplay(env.observation_space, env.action_space)\n",
    "env.run(agent, episodes=300, print_delay=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.plot_diagnostics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More ideas\n",
    "* Use an other environment providing pictures, replace the network by a pre-trained ResNet-50 and learn a policy to play your favorite Atari game (can easily take days of computation if not done asynchroneously on several V100 as described by https://github.com/dgriff777/rl_a3c_pytorch),\n",
    "* Visualize important areas of the pictures for taking the decision using saliency maps computed by guided backpropagation as shown by http://arxiv.org/abs/1412.6806.\n",
    "* Go to policy gradient methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
